{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***Project Overview***\n",
        "\n",
        "In this project, I am developing a machine learning service that automatically generates tags for StackOverflow questions. This task involves processing text data, classifying it, and using machine learning techniques to automatically label questions based on their content.\n",
        "\n",
        "**Objective:** To predict tags for StackOverflow questions using their text content.\n",
        "\n",
        "**Dataset for model training:**\n",
        "Dataset with StackOverflow questions, available on Kaggle: https://www.kaggle.com/datasets/muhammedabdulazeem/500k-stackoverflow-questions/data. This dataset contains questions, answers, and tags associated with the questions. For this task, I only use questions and their associated tags.\n",
        "\n",
        "**Input:** Text of a StackOverflow question (both title and body).\n",
        "\n",
        "**Output:** List of tags most relevant to the question.\n",
        "\n",
        "**Approach chosen to solve:**\n",
        "1. **Data Exploration and Preprocessing:** I start with a dataset comprising 500k StackOverflow questions, each with associated tags. The preprocessing steps include cleaning text, tokenization, and transforming tags into a format suitable for multi-label classification.\n",
        "\n",
        "2. **Model Training with Transfer Learning:** Leveraging BERT, a pre-trained transformer model renowned for its effectiveness in NLP tasks, I fine-tune it for the specific tag prediction task. This approach allows to benefit from BERT's understanding of language nuances without the need for extensive computational resources.\n",
        "\n",
        "3. **MLOps Integration:** The project incorporates MLOps practices to streamline the machine learning workflow. I use MLflow for experiment tracking, allowing to log parameters, metrics, and models. For a more comprehensive overview, Neptune.ai is utilized, offering advanced experiment tracking and visualization capabilities."
      ],
      "metadata": {
        "id": "yRyI3T4XsI5-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading data**"
      ],
      "metadata": {
        "id": "pBABRfs-ilGR"
      }
    },
    {
      "source": [
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'stacksample:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F265%2F726723%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240310%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240310T122334Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D7378857a4ec1011323f7010c07ccaecccdf51791e2adeb1da6d85a2927fd82823f53ea1e9a9aab4b44b90ecc4ae37b520e66212ab21e92eaed5fa5681a7397432f96fdeb08e8e438e5eaea5a62d7c9a5df1bb8ecad586d02b65def5a597961e81fdfd753bade22e650fd9668d724a2822e346b438bd2ac6ca8474fe95cbd2353a80b35a4bc179b860148026976d23d9231842bbeb576cbec6a76ff58b83956bdf31448201943114ad0578dda48a0f5951782e8f247d4d3288e4d89bec94674330f44d788083ef253a29dfdea86bfc4791d857f04edc507150278110dd923e5833cb8ecc6a1c2f806b35895f2b3ca08951d51da3fee39e8b938f26a8b0b543838,500k-stackoverflow-questions:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F1232267%2F2056442%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240310%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240310T122334Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D7763511eb86c99a3e91033e90ed861fa944f7925f753eb4a6f05996258d83828e80962c407379a310d782887aa38acdfc21f1e3260dbe41902b9d88db310cb32c092d39ad2ce39946053ebb7ba0b7bb00f345f57d2bca21cf79f2f2d15aaf93b30f1ebfad57b267fae59f67768d6e79109e37e2d5b756baf19979ba414daf301374f352e605865a5dd125dddb09cec83de15672b0a95a7d84b10f1a9d5d2fe26c04bd871e24ba8c79436d31c4c7c69c5b9d4e9bc37fbd0df5f60d2aa26854545ab758692d701c9bf6065c1113a600dd3bb27dc407ae289f723c2376811daed553c56784bfdeb7e6d91989d0cdb55366e2fe7de55b2b1286fcd4086e3a1f235c6'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "oKnsoArQUZkB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81051c5d-2148-4cb6-bf08-2b96f7f481bf"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading stacksample, 1189724316 bytes compressed\n",
            "[==================================================] 1189724316 bytes downloaded\n",
            "Downloaded and uncompressed: stacksample\n",
            "Downloading 500k-stackoverflow-questions, 50856690 bytes compressed\n",
            "[==================================================] 50856690 bytes downloaded\n",
            "Downloaded and uncompressed: 500k-stackoverflow-questions\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Exploration and Preprocessing**"
      ],
      "metadata": {
        "id": "dTwf5_sBKMfz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I explore the dataset to understand its structure and then preprocess the text data to make it suitable for training a machine learning model."
      ],
      "metadata": {
        "id": "cafd_H2TLPSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "questions_path = '/kaggle/input/500k-stackoverflow-questions/questions.csv'\n",
        "df = pd.read_csv(questions_path)\n",
        "\n",
        "print(df.head())\n",
        "print(df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVnFIpRHCVYy",
        "outputId": "fdfb7de1-8253-46f3-cd3d-1cd0161d7a6b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                         date  \\\n",
            "0  2021-03-24 11:01:18.812726   \n",
            "1  2021-03-24 11:01:18.814679   \n",
            "2  2021-03-24 11:01:18.817728   \n",
            "3  2021-03-24 11:01:18.818696   \n",
            "4  2021-03-24 11:01:18.820720   \n",
            "\n",
            "                                               links  \\\n",
            "0  /questions/66775243/how-to-display-jalali-date...   \n",
            "1  /questions/66775242/automate-creating-of-sales...   \n",
            "2  /questions/66775240/java-jar-error-for-spring-...   \n",
            "3  /questions/66775238/not-able-to-run-unfoldingm...   \n",
            "4  /questions/66775237/serverless-graphql-lambda-...   \n",
            "\n",
            "                                           questions  \\\n",
            "0  How to display jalali date in to view in Codei...   \n",
            "1     Automate creating of sales order in Zoho Books   \n",
            "2          java jar error for spring boot applicaton   \n",
            "3               Not able to run UnfoldingMap library   \n",
            "4  Serverless Graphql Lambda hard to understand t...   \n",
            "\n",
            "                                                tags        time  \n",
            "0            php,codeigniter,date,gregorian-calendar  2 mins ago  \n",
            "1                               python-3.x,zohobooks  2 mins ago  \n",
            "2                            java,spring,spring-boot  3 mins ago  \n",
            "3          java,eclipse,dictionary,core,unfoldingmap  3 mins ago  \n",
            "4  typescript,webpack,error-handling,graphql,serv...  3 mins ago  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 529804 entries, 0 to 529803\n",
            "Data columns (total 5 columns):\n",
            " #   Column     Non-Null Count   Dtype \n",
            "---  ------     --------------   ----- \n",
            " 0   date       529804 non-null  object\n",
            " 1   links      529804 non-null  object\n",
            " 2   questions  529804 non-null  object\n",
            " 3   tags       529804 non-null  object\n",
            " 4   time       529804 non-null  object\n",
            "dtypes: object(5)\n",
            "memory usage: 20.2+ MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing steps include cleaning text data, tokenization, and splitting the data into training and testing sets."
      ],
      "metadata": {
        "id": "H8ZIYY84LV_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\@w+|\\#','', text)\n",
        "    text = re.sub(r'[^\\w\\s]','', text)\n",
        "    text = text.strip()\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered_words = [word for word in tokens if word not in stopwords.words('english')]\n",
        "    return \" \".join(filtered_words)\n",
        "\n",
        "df['questions_cleaned'] = df['questions'].apply(lambda x: clean_text(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bB9lUOaRCVlE",
        "outputId": "6f4539a5-9795-443e-86ca-414db2829d86"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Process tags for multi-label binarization\n",
        "df['tags_list'] = df['tags'].apply(lambda x: x.split('|'))\n",
        "mlb = MultiLabelBinarizer()\n",
        "tags_encoded = mlb.fit_transform(df['tags_list'])\n",
        "tags_df = pd.DataFrame(tags_encoded, columns=mlb.classes_)\n",
        "df = df.join(pd.DataFrame(mlb.fit_transform(df.pop('tags')), columns=mlb.classes_, index=df.index))"
      ],
      "metadata": {
        "id": "vT46fNiNQlvk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the dataset into training and testing set\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['questions_cleaned'], df[df.columns.difference(['date', 'links', 'questions', 'questions_cleaned', 'time'])], test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "KIyTDVviQ5LC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Training with Transfer Learning**"
      ],
      "metadata": {
        "id": "GcCszVq3LbeN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I use a pre-trained BERT model from the Hugging Face library as a base model for transfer learning."
      ],
      "metadata": {
        "id": "xjjtty1IL0Ci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting Up the Model:**"
      ],
      "metadata": {
        "id": "IPV6EvK8MVn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "\n",
        "# Load pre-trained BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Prepare model for transfer learning\n",
        "input_ids = Input(shape=(512,), dtype='int32')\n",
        "attention_masks = Input(shape=(512,), dtype='int32')\n",
        "\n",
        "output = bert(input_ids, attention_mask=attention_masks)\n",
        "output = output[1]\n",
        "output = Dense(y_train.shape[1], activation='sigmoid')(output)\n",
        "\n",
        "model = Model(inputs=[input_ids, attention_masks], outputs=output)\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=2e-5), loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "mXSOh-0BCVv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preparing Data for BERT:**"
      ],
      "metadata": {
        "id": "UazFjoe3Mole"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data_for_bert(texts, tokenizer, max_length=512):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for text in texts:\n",
        "        encoded = tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='tf',\n",
        "        )\n",
        "        input_ids.append(encoded['input_ids'])\n",
        "        attention_masks.append(encoded['attention_mask'])\n",
        "\n",
        "    return [tf.concat(input_ids, 0), tf.concat(attention_masks, 0)]\n",
        "\n",
        "X_train_ids, X_train_masks = prepare_data_for_bert(X_train, tokenizer)\n",
        "X_test_ids, X_test_masks = prepare_data_for_bert(X_test, tokenizer)"
      ],
      "metadata": {
        "id": "BVFK8JCOEhtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Training:**"
      ],
      "metadata": {
        "id": "wO4c_8c0MwC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    [X_train_ids, X_train_masks], y_train,\n",
        "    validation_data=([X_test_ids, X_test_masks], y_test),\n",
        "    epochs=3,\n",
        "    batch_size=16\n",
        ")"
      ],
      "metadata": {
        "id": "3TzrspZyEh4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MLOps Integration**"
      ],
      "metadata": {
        "id": "aIq0drKTM3pA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MLflow for Experiment Tracking:**"
      ],
      "metadata": {
        "id": "FgJ2IOFmM80Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "from mlflow import log_metric, log_param, log_artifacts\n",
        "\n",
        "mlflow.set_experiment('stackoverflow_tags_prediction')\n",
        "\n",
        "with mlflow.start_run():\n",
        "    mlflow.tensorflow.log_model(tf_model=model, artifact_path='bert_model')\n",
        "    log_param(\"epochs\", 3)\n",
        "    log_param(\"batch_size\", 16)\n",
        "    log_metric(\"loss\", history.history['loss'][-1])\n",
        "    log_metric(\"accuracy\", history.history['accuracy'][-1])"
      ],
      "metadata": {
        "id": "prIz66uSCV6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neptune.ai Integration:**"
      ],
      "metadata": {
        "id": "IcFhElkMNRw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import neptune\n",
        "\n",
        "run = neptune.init_run(\n",
        "    project=\"alishanoskova/lsml2\",\n",
        "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJmYjEzNjMyYS1iN2M0LTQ0YjUtODYyYS0zMjY0NDdiMTBiOWYifQ==\",\n",
        ")\n",
        "\n",
        "# Log parameters\n",
        "run[\"parameters\"] = {\"epochs\": 3, \"batch_size\": 16}\n",
        "\n",
        "# Log metrics\n",
        "for epoch in range(len(history.history['loss'])):\n",
        "    run[\"train/loss\"].log(history.history['loss'][epoch])\n",
        "    run[\"train/accuracy\"].log(history.history['accuracy'][epoch])\n",
        "    run[\"val/loss\"].log(history.history['val_loss'][epoch])\n",
        "    run[\"val/accuracy\"].log(history.history['val_accuracy'][epoch])\n",
        "\n",
        "run.stop()"
      ],
      "metadata": {
        "id": "XEAydTrREpjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_"
      ],
      "metadata": {
        "id": "02uu8SNBwQED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Making predictions and evaluating the model:**"
      ],
      "metadata": {
        "id": "JYFFmn35N5NK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "predictions = model.predict([X_test_ids, X_test_masks])\n",
        "\n",
        "preds_binary = np.where(predictions > 0.5, 1, 0)\n",
        "\n",
        "accuracy = accuracy_score(y_test, preds_binary)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(y_test, preds_binary, average='micro')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1-score: {f1}')"
      ],
      "metadata": {
        "id": "er9smS8_Epwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_"
      ],
      "metadata": {
        "id": "nfJWLZ7aJEKn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code for Serving:**"
      ],
      "metadata": {
        "id": "DImsi57SCoaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer\n",
        "import numpy as np\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# model = tf.keras.models.load_model('model')\n",
        "\n",
        "def prepare_data_for_bert(text, tokenizer):\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens = True,\n",
        "        max_length = 512,\n",
        "        pad_to_max_length = True,\n",
        "        return_attention_mask = True,\n",
        "        return_tensors = 'tf',\n",
        "    )\n",
        "\n",
        "    input_id = encoded_dict['input_ids']\n",
        "    attention_mask = encoded_dict['attention_mask']\n",
        "    return input_id, attention_mask\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    data = request.get_json(force=True)\n",
        "    text = data['text']\n",
        "    input_id, attention_mask = prepare_data_for_bert(text, tokenizer)\n",
        "    predictions = model.predict([input_id, attention_mask])\n",
        "\n",
        "    threshold = 0.5\n",
        "    tag_indexes = np.where(predictions > threshold)\n",
        "    predicted_tags = [mlb.classes_[i] for i in tag_indexes[1]]\n",
        "    return jsonify(predicted_tags)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)"
      ],
      "metadata": {
        "id": "FTdcHL_6CnYJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}